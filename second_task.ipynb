{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientDescentClassifier:\n",
    "    def __init__(self, learning_rate=0.01, lambda1=0.1, lambda2=0.1, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.epochs = epochs\n",
    "        self.coef_ = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'lambda1': self.lambda1,\n",
    "            'lambda2': self.lambda2,\n",
    "            'epochs': self.epochs\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "\n",
    "    def hinge_loss(self, margin):\n",
    "        return np.maximum(0, 1 - margin)\n",
    "\n",
    "    def perceptron_loss(self, margin):\n",
    "        return np.maximum(0, -margin)\n",
    "\n",
    "    def logistic_loss(self, margin):\n",
    "        return np.log(1 + np.exp(-margin))\n",
    "\n",
    "    def fit(self, X, y, loss_type=\"hinge\"):\n",
    "        # Преобразуем X в плотную матрицу\n",
    "        if hasattr(X, 'toarray'):\n",
    "            X = X.toarray()\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "\n",
    "        # Преобразуем y в массив и задаем его размерность\n",
    "        y = np.array(y).reshape(-1)\n",
    "\n",
    "        loss_functions = {\n",
    "            \"hinge\": self.hinge_loss,\n",
    "            \"perceptron\": self.perceptron_loss,\n",
    "            \"logistic\": self.logistic_loss\n",
    "        }\n",
    "        loss_func = loss_functions[loss_type]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            margins = y * (X @ self.coef_)\n",
    "            loss = loss_func(margins)\n",
    "\n",
    "            grad = -np.mean((X.T * y * (loss > 0)).T, axis=0) + self.lambda1 * np.sign(\n",
    "                self.coef_) + self.lambda2 * self.coef_\n",
    "\n",
    "            self.coef_ -= self.learning_rate * grad\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X @ self.coef_)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:02:09.363918Z",
     "start_time": "2024-11-15T17:02:09.359861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RidgeRegression:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Добавляем столбец единиц для учета свободного члена\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        # Формула для Ridge регрессии: (X^T X + alpha * I)^(-1) X^T y\n",
    "        I = np.eye(X.shape[1])\n",
    "        I[0, 0] = 0  # не регуляризируем свободный член\n",
    "        self.coef_ = np.linalg.inv(X.T @ X + self.alpha * I) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        return X @ self.coef_"
   ],
   "id": "b7d33e0313a14f21",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, C=1.0, kernel=\"linear\", gamma=1, degree=3):\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.degree = degree\n",
    "        self.alphas = None\n",
    "        self.b = 0\n",
    "        self.support_vectors = None\n",
    "\n",
    "    def _linear_kernel(self, x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "\n",
    "    def _polynomial_kernel(self, x1, x2):\n",
    "        return (1 + np.dot(x1, x2)) ** self.degree\n",
    "\n",
    "    def _rbf_kernel(self, x1, x2):\n",
    "        return np.exp(-self.gamma * np.linalg.norm(x1 - x2) ** 2)\n",
    "\n",
    "    def _kernel(self, x1, x2):\n",
    "        if self.kernel == \"linear\":\n",
    "            return self._linear_kernel(x1, x2)\n",
    "        elif self.kernel == \"polynomial\":\n",
    "            return self._polynomial_kernel(x1, x2)\n",
    "        elif self.kernel == \"rbf\":\n",
    "            return self._rbf_kernel(x1, x2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.alphas = np.zeros(n_samples)\n",
    "        self.b = 0\n",
    "\n",
    "        def compute_L_H(alpha_i, alpha_j, y_i, y_j):\n",
    "            if y_i != y_j:\n",
    "                return (max(0, alpha_j - alpha_i), min(self.C, self.C + alpha_j - alpha_i))\n",
    "            else:\n",
    "                return (max(0, alpha_i + alpha_j - self.C), min(self.C, alpha_i + alpha_j))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.array([np.sum(self.alphas * self.support_vectors[:, i]) for i in range(len(X))]))"
   ],
   "id": "e6e2388411c4218"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
